{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Groups message scraper\n",
    "\n",
    "This notebook provides code that will dowload messages from *private* Yahoo Groups. Another notebook `YahooScra.jpynb` uses [Selenium](http://selenium-python.readthedocs.io/). This notebook uses [reqeusts](http://docs.python-requests.org/en/latest/user/install/).\n",
    "\n",
    "Another GitHub.com repository ([YahooGroups-Archiver](https://github.com/andrewferguson/YahooGroups-Archiver)) provided guidance related to accessing *private* Yahoo Groups. After logging into the private group you need to provide two pieces of information from the cookies:\n",
    "\n",
    "> Cookie information can be found through the use of a plug-in for\n",
    "> your web browser. (I use 'Cookie Manager' on FireFox, although\n",
    "> there are many other options for FireFox and other browsers). The\n",
    "> two cookies you are looking for are called Y and T, and they are \n",
    "> linked to the domain yahoo.com. Extract the data from these \n",
    "> cookies, and paste it into the appropriate variables... a cookie\n",
    "> will expire after a certain amount of time, which varies between \n",
    "> computers. This means that you may have to re-fetch the Y and T \n",
    "> cookie data every few days, or you will not be able to archive \n",
    "> private groups. ([YahooGroups-Archiver](https://github.com/andrewferguson/YahooGroups-Archiver))\n",
    "\n",
    "(Last tested April 21, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get groupname from user.\n",
    "grp_name = input(prompt='What is the group name you seek to scrape?  ')\n",
    "# Give grp_name a default if no input.\n",
    "if grp_name == '':\n",
    "    grp_name = 'concatenative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables will take the cookies from 'Cookie Manager' discussed above as strings.\n",
    "cookie_T = ''\n",
    "cookie_Y = ''\n",
    "\n",
    "# Define a list to log successful attempts.\n",
    "fetch_log = []\n",
    "\n",
    "# Iterate through the private group messages. The most recent group message number \n",
    "# can be found at: https://groups.yahoo.com/api/v1/groups/<groupname>/messages?count=10\n",
    "# Parse the results to discern the most recent message number.\n",
    "for i in range(120,100,-1):\n",
    "    fetch_log.append(\n",
    "        ('{} Working on message {}.'.format(\n",
    "            str(datetime.datetime.now()), str(i))))\n",
    "    # For demonstration purposes use a generic groupsuch as 'concatenative'\n",
    "    post = requests.get(\n",
    "        ''.join(\n",
    "            (r'https://groups.yahoo.com/api/v1/groups/', grp_name, r'/messages/', str(i), r'/')),\n",
    "             cookies={'T': cookie_T, 'Y': cookie_Y})\n",
    "    \n",
    "    # Yahoo Groups api returns JSON.\n",
    "    post_parsed = json.loads(post.text)\n",
    "    \n",
    "    # The api result uses html in the messageBody.\n",
    "    soup = BeautifulSoup(post_parsed['ygData']['messageBody'], 'html.parser')\n",
    "    soupstring = soup.get_text()\n",
    "    \n",
    "    # Optionally remove comments from html\n",
    "    # pullstring = soupstring[soupstring.find('<!--'):soupstring.find('-->')+3]\n",
    "    # cleanstring = soupstring.replace(pullstring,'')\n",
    "    \n",
    "    # Save the message body as a .txt file.\n",
    "    post_file = open(\n",
    "        os.path.join(\n",
    "            'msgs', ''.join(\n",
    "                (grp_name + '_' + str(i) + r'_post.txt'))), 'w', encoding='utf-8')\n",
    "    # post_file.write(cleanstring)\n",
    "    post_file.write(soupstring)\n",
    "    post_file.close()\n",
    "    \n",
    "    # Save the api result as a .json file.\n",
    "    json_file = open(\n",
    "        os.path.join(\n",
    "            'msgs', ''.join(\n",
    "                (grp_name + '_' + str(i) + r'_json.json'))), 'w', encoding='utf-8')\n",
    "    json_file.write(post.text)\n",
    "    json_file.close()\n",
    "    \n",
    "    # Optionally pause to assist in avoiding CAPTCHA and other anti-robot features.\n",
    "    time.sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fetch_log for later reference.\n",
    "with open(grp_name + '_fetch_log_' + \n",
    "        str(datetime.datetime.now())[2:16].replace(\" \", \"-\").replace(\":\",\"\") + \n",
    "        '.log', mode='w') as logfile:\n",
    "            print('This is the log of fetched messages file from {}'.format(\n",
    "                str(datetime.datetime.now())), file=logfile)\n",
    "            print('Yahoo Group name {}.'.format(grp_name), file=logfile)\n",
    "            for fetch_line in fetch_log:\n",
    "                print(fetch_line, file = logfile)\n",
    "logfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get location of messages to compile.\n",
    "folder_location = input(prompt='What is the of messages (no input will default to msgs)?  ')\n",
    "# Give grp_name a default if no input.\n",
    "if folder_location == '':\n",
    "    folder_location = 'msgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset from the files created above.\n",
    "\n",
    "# Define a list to log errors.\n",
    "error_log  = []\n",
    "\n",
    "# Define a list to hold structured data.\n",
    "grandlist = []\n",
    "\n",
    "# Iterate through the message files.\n",
    "for i in range(32811,32800,-1):\n",
    "    try:\n",
    "        work_file = open(\n",
    "            os.path.join(\n",
    "                folder_location, ''.join((grp_name, '_', str(i), '_json.json'))),'r', encoding='utf-8')\n",
    "        work_parse = json.loads(work_file.read())\n",
    "        \n",
    "        # Define list to hold current record.\n",
    "        mylist = []\n",
    "        mylist = [\n",
    "            work_parse['ygData']['userId'],\n",
    "            work_parse['ygData']['authorName'],\n",
    "            work_parse['ygData']['subject'],\n",
    "            work_parse['ygData']['postDate'],\n",
    "            str(datetime.datetime.fromtimestamp(\n",
    "                int(work_parse['ygData']['postDate'])).strftime('%Y-%m-%d %H:%M:%S')),\n",
    "            work_parse['ygData']['msgId'],\n",
    "            work_parse['ygData']['prevInTopic'],\n",
    "            work_parse['ygData']['nextInTopic'],\n",
    "            work_parse['ygData']['prevInTime'],\n",
    "            work_parse['ygData']['nextInTime'],\n",
    "            work_parse['ygData']['topicId'],\n",
    "            work_parse['ygData']['numMessagesInTopic']]\n",
    "        work_file.close()\n",
    "        \n",
    "        # Optionally add the message body to the current observation.\n",
    "        # My use case involved adding the message body using Stata instead of Python.\n",
    "        work_file = open(\n",
    "            os.path.join(\n",
    "                folder_location, ''.join((grp_name, '_', str(i), '_post.txt'))), 'r', encoding='utf-8')\n",
    "        mylist.append(work_file.read())\n",
    "        work_file.close()\n",
    "\n",
    "        # Add the current observation to the structured data set.\n",
    "        grandlist.append(mylist)\n",
    "    except FileNotFoundError:\n",
    "        # If file not found, provie output and log error.\n",
    "        print('Message number ' + str(i) + ' - Not found.')\n",
    "        error_log.append('FileNotFoundError. Message number {}.'.format(str(i)))\n",
    "    except KeyError:\n",
    "        # If any of the JSON keys (variables) not found, provie output and log error.\n",
    "        print('Message number ' + str(i) + ' - KeyError.')\n",
    "        error_log.append('KeyError. Message number '.format(str(i)))\n",
    "    except OSError:\n",
    "        # If OSError, provie output and log error.\n",
    "        print('Message number ' + str(i) + ' - OSError.')\n",
    "        error_log.append('OSError. Message number '.format(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error_log for later reference.\n",
    "with open(grp_name + '_err_log_' + \n",
    "        str(datetime.datetime.now())[2:16].replace(\" \", \"-\").replace(\":\",\"\") +\n",
    "        '.log', mode='w') as logfile:\n",
    "            print('This is the error log file from {}'.format(\n",
    "                str(datetime.datetime.now())), file = logfile)\n",
    "            print('Yahoo Group name {}.'.format(grp_name), file = logfile)\n",
    "            for error_line in error_log:\n",
    "                print(error_line, file = logfile)\n",
    "logfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put structured data into a Pandas dataframe.\n",
    "grand_df = DataFrame(grandlist, \n",
    "                     columns=['userId','authName','subject','Unix','Date',\n",
    "                              'msgId','preInTpc','nxtInTpc','preInTime',\n",
    "                              'nxtInTime','topicId','MssgsInTopic','msgBody'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results.\n",
    "grand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "grand_df.to_csv(grp_name + '_messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Stata\n",
    "# Problems on this. See: https://github.com/pandas-dev/pandas/issues/16450\n",
    "grand_df.to_stata(grp_name + '_messages.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "writer = pd.ExcelWriter(grp_name + '_messages.xlsx', engine='xlsxwriter')\n",
    "grand_df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
