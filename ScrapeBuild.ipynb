{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Groups message scraper\n",
    "\n",
    "This notebook provides code that will dowload messages from *private* Yahoo Groups. Another notebook `YahooScra.jpynb` uses [Selenium](http://selenium-python.readthedocs.io/). This notebook uses [reqeusts](http://docs.python-requests.org/en/latest/user/install/).\n",
    "\n",
    "Another GitHub.com repository ([YahooGroups-Archiver](https://github.com/andrewferguson/YahooGroups-Archiver)) provided guidance related to accessing *private* Yahoo Groups. After logging into the private group you need to provide two pieces of information from the cookies:\n",
    "\n",
    "> Cookie information can be found through the use of a plug-in for\n",
    "> your web browser. (I use 'Cookie Manager' on FireFox, although\n",
    "> there are many other options for FireFox and other browsers). The\n",
    "> two cookies you are looking for are called Y and T, and they are \n",
    "> linked to the domain yahoo.com. Extract the data from these \n",
    "> cookies, and paste it into the appropriate variables... a cookie\n",
    "> will expire after a certain amount of time, which varies between \n",
    "> computers. This means that you may have to re-fetch the Y and T \n",
    "> cookie data every few days, or you will not be able to archive \n",
    "> private groups. ([YahooGroups-Archiver](https://github.com/andrewferguson/YahooGroups-Archiver))\n",
    "\n",
    "(Last tested April 21, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get groupname from user.\n",
    "grp_name = input(prompt='What is the group name you seek to scrape?  ')\n",
    "# Give grp_name a default if no input.\n",
    "if grp_name == '':\n",
    "    grp_name = 'concatenative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables will take the cookies from 'Cookie Manager' discussed above as strings.\n",
    "cookie_T = ''\n",
    "cookie_Y = ''\n",
    "\n",
    "# Define a list to log successful attempts.\n",
    "fetch_log = []\n",
    "\n",
    "# Iterate through the private group messages. The most recent group message number \n",
    "# can be found at: https://groups.yahoo.com/api/v1/groups/<groupname>/messages?count=10\n",
    "# Parse the results to discern the most recent message number.\n",
    "for i in range(1737,0,-1):\n",
    "    fetch_log.append(\n",
    "        ('{} Working on message {}.'.format(\n",
    "            str(datetime.datetime.now()), str(i))))\n",
    "    # For demonstration purposes use a generic groupsuch as 'concatenative'\n",
    "    \n",
    "    # Alternate code approach available.\n",
    "    # s = requests.Session()\n",
    "    # post = s.get('https://groups.yahoo.com/api/v1/groups/' + grp_name + '/messages/' + str(i) + '/', cookies={'T': cookie_T, 'Y': cookie_Y})\n",
    "\n",
    "    post = requests.get(\n",
    "        'https://groups.yahoo.com/api/v1/groups/' + grp_name + '/messages/' + str(i) + '/', \n",
    "        cookies={'T': cookie_T, 'Y': cookie_Y})\n",
    "\n",
    "    # Yahoo Groups api returns JSON.\n",
    "    post_parsed = json.loads(post.text)\n",
    "    \n",
    "    # The api result uses html in the messageBody; remove html tags from messageBody.\n",
    "    try:\n",
    "        soup = BeautifulSoup(post_parsed['ygData']['messageBody'], 'html.parser')\n",
    "        soupstring = soup.get_text()\n",
    "    # If no message found for that index; populate with message not found.\n",
    "    except KeyError:\n",
    "        soupstring = 'Post number {} was not found.'.format(str(i))\n",
    "    \n",
    "    # Save the message body as a .txt file.\n",
    "    post_file = open(\n",
    "        os.path.join(\n",
    "            'msgs', ''.join(\n",
    "                (grp_name + '_' + str(i) + r'_post.txt'))), 'w', encoding='utf-8')\n",
    "    post_file.write(soupstring)\n",
    "    post_file.close()\n",
    "    \n",
    "    # Save the api result as a .json file.\n",
    "    json_file = open(\n",
    "        os.path.join(\n",
    "            'msgs', ''.join(\n",
    "                (grp_name + '_' + str(i) + r'_json.json'))), 'w', encoding='utf-8')\n",
    "    json_file.write(post.text)\n",
    "    json_file.close()\n",
    "    \n",
    "    # Optionally pause to assist in avoiding CAPTCHA and other anti-robot features.\n",
    "    time.sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fetch_log for later reference.\n",
    "with open(grp_name + '_fetch_log_' + \n",
    "        str(datetime.datetime.now())[2:16].replace(\" \", \"-\").replace(\":\",\"\") + \n",
    "        '.log', mode='w') as logfile:\n",
    "            print('This is the log of fetched messages file from {}'.format(\n",
    "                str(datetime.datetime.now())), file=logfile)\n",
    "            print('Yahoo Group name {}.'.format(grp_name), file=logfile)\n",
    "            for fetch_line in fetch_log:\n",
    "                print(fetch_line, file = logfile)\n",
    "logfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get location of messages to compile.\n",
    "folder_location = input(prompt='What is the location of messages (no input will default to msgs)?  ')\n",
    "# Give grp_name a default if no input.\n",
    "if folder_location == '':\n",
    "    folder_location = 'msgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset from the files created above.\n",
    "\n",
    "# Define a list to log errors.\n",
    "error_log  = []\n",
    "\n",
    "# Define a list to hold structured data.\n",
    "grandlist = []\n",
    "\n",
    "# Iterate through the message files.\n",
    "for i in range(33102,0,-1):\n",
    "    try:\n",
    "        work_file = open(\n",
    "            os.path.join(\n",
    "                folder_location, grp_name + '_' + str(i) + '_json.json'), 'r', encoding='utf-8')\n",
    "        work_parse = json.loads(work_file.read())\n",
    "        \n",
    "        # Define list to hold current record.\n",
    "        mylist = []\n",
    "        mylist = [\n",
    "            work_parse['ygData']['userId'],\n",
    "            work_parse['ygData']['authorName'],\n",
    "            work_parse['ygData']['subject'],\n",
    "            work_parse['ygData']['postDate'],\n",
    "            str(datetime.datetime.fromtimestamp(\n",
    "                int(work_parse['ygData']['postDate'])).strftime('%Y-%m-%d %H:%M:%S')),\n",
    "            work_parse['ygData']['msgId'],\n",
    "            work_parse['ygData']['prevInTopic'],\n",
    "            work_parse['ygData']['nextInTopic'],\n",
    "            work_parse['ygData']['prevInTime'],\n",
    "            work_parse['ygData']['nextInTime'],\n",
    "            work_parse['ygData']['topicId'],\n",
    "            work_parse['ygData']['numMessagesInTopic'],\n",
    "            work_parse['ygData']['messageBody']]\n",
    "        work_file.close()\n",
    "        \n",
    "        work_file = open(\n",
    "            os.path.join(\n",
    "                folder_location, grp_name + '_' + str(i) + '_post.txt'), 'r', encoding='utf-8')\n",
    "        mylist.append(work_file.read())\n",
    "        work_file.close()\n",
    "\n",
    "        # Add the current observation to the structured data set.\n",
    "        grandlist.append(mylist)\n",
    "    except FileNotFoundError:\n",
    "        # If file not found, provide output and log error.\n",
    "        print('Message number ' + str(i) + ' - Not found.')\n",
    "        error_log.append('FileNotFoundError. Message number {}.'.format(str(i)))\n",
    "    except KeyError:\n",
    "        # If any of the JSON keys (variables) not found, provide output and log error.\n",
    "        print('Message number ' + str(i) + ' - KeyError.')\n",
    "        error_log.append('KeyError. Message number '.format(str(i)))\n",
    "    except OSError:\n",
    "        # If OSError, provide output and log error.\n",
    "        print('Message number ' + str(i) + ' - OSError.')\n",
    "        error_log.append('OSError. Message number '.format(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error_log for later reference.\n",
    "with open(grp_name + '_err_log_' + \n",
    "        str(datetime.datetime.now())[2:16].replace(\" \", \"-\").replace(\":\",\"\") +\n",
    "        '.log', mode='w') as logfile:\n",
    "            print('This is the error log file from {}'.format(\n",
    "                str(datetime.datetime.now())), file = logfile)\n",
    "            print('Yahoo Group name {}.'.format(grp_name), file = logfile)\n",
    "            for error_line in error_log:\n",
    "                print(error_line, file = logfile)\n",
    "logfile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column list (With message body)\n",
    "col_list = ['userId','authName','subject','Unix','Date',\n",
    "            'msgId','preInTpc','nxtInTpc','preInTime',\n",
    "            'nxtInTime','topicId','MsgsInTopic','msgRaw','msgTxt']\n",
    "\n",
    "# Put structured data into a Pandas dataframe.\n",
    "grand_df = DataFrame(grandlist, columns=col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results.\n",
    "grand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "grand_df.to_csv(grp_name + '_messages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_comments(commented_text):\n",
    "    # Remove comments from html\n",
    "    pull_string = commented_text[commented_text.find('<!--'):commented_text.find('-->')+3]\n",
    "    clean_string = commented_text.replace(pull_string,'')\n",
    "    return(clean_string)\n",
    "\n",
    "def remove_unicode(uni_txt):\n",
    "    # Replace common Unicode chars with reasonable substitutes\n",
    "    uni_txt = re.sub('[\\u2022]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2014]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2013]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u0335]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2011]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u1427]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2027]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2015]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2212]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u25cf]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u25aa]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00B7]', '-', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u201a]', \"'\", uni_txt)\n",
    "    uni_txt = re.sub('[\\u2019]', \"'\", uni_txt)\n",
    "    uni_txt = re.sub('[\\u2018]', \"'\", uni_txt)\n",
    "    uni_txt = re.sub('[\\u200b]', \"'\", uni_txt)\n",
    "    uni_txt = re.sub('[\\u2026]', \"'\", uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u201c]', '\"', uni_txt)\n",
    "    uni_txt = re.sub('[\\u201d]', '\"', uni_txt)\n",
    "    uni_txt = re.sub('[\\u04cf]', \"'\", uni_txt)\n",
    "\n",
    "    uni_txt = re.sub('[\\u2002]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\ufddf]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\ufffd]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\U0001f603]', ' ', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u2028]', '... ...', uni_txt)\n",
    "    uni_txt = re.sub('[\\u263a]', ' smile ', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u25ba]', '>', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2039]', '<', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2329]', '<', uni_txt)\n",
    "    uni_txt = re.sub('[\\u232A]', '>', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u002D]', '-', uni_txt)\n",
    "    uni_txt = re.sub('[\\u03c0]', 'pi', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0d8]', '|?|', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2502]', '|', uni_txt)\n",
    "    uni_txt = re.sub('[\\u0160]', 'S', uni_txt)\n",
    "    uni_txt = re.sub('[\\u02dc]', '~', uni_txt)\n",
    "    uni_txt = re.sub('[\\u0153]', 'oe', uni_txt)\n",
    "    uni_txt = re.sub('[\\ufb01]', 'fi', uni_txt)\n",
    "    uni_txt = re.sub('[\\u201e]', '\"', uni_txt)\n",
    "    uni_txt = re.sub('[\\u0192]', 'f', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2030]', '%', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00AE]', '(r)', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00BD]', '1/2', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00A7]', 'Section', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u2010]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\ufffc]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\u200e]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf050]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\u25a1]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf097]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\u20AC]', ' ', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2122]', ' ', uni_txt)\n",
    "    \n",
    "    uni_txt = re.sub('[\\u2282]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0a7]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0a7]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0a7]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0a7]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u2020]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00AC]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u00B8]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uF1FF]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u3050]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u98B5]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u11CF]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u82BB]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uAA00]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uBD00]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u0bce]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf022]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\u221d]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\uf0a7]', '', uni_txt)\n",
    "    uni_txt = re.sub('[\\U0010003e]', '', uni_txt)\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    uni_txt = uni_txt.strip()\n",
    "    \n",
    "    # Remove other miscellaneous characters\n",
    "    ret_txt = ''\n",
    "    for item in uni_txt:\n",
    "        ret_txt += item if len(item.encode(encoding='utf_8')) == 1 else ''\n",
    "    \n",
    "    # Remove <!--   --> style comments from html text\n",
    "    ret_txt = remove_comments(ret_txt)\n",
    "    \n",
    "    return(ret_txt)\n",
    "\n",
    "grand_df['msgRaw'] = grand_df['msgRaw'].apply(remove_unicode)\n",
    "grand_df['msgTxt'] = grand_df['msgTxt'].apply(remove_unicode)\n",
    "grand_df['subject'] = grand_df['subject'].apply(remove_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Stata\n",
    "# Problems on this. See: https://github.com/pandas-dev/pandas/issues/16450\n",
    "grand_df.to_stata(grp_name + '_messages.dta', version=117, convert_strl=['subject','msgRaw','msgTxt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "writer = pd.ExcelWriter(grp_name + '_messages.xlsx', engine='xlsxwriter')\n",
    "grand_df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
